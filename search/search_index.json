{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"hai","text":"<p>A REPL for hackers using LLMs.</p> <p></p>"},{"location":"#highlights","title":"Highlights","text":"<ul> <li>\u26a1\ufe0f Starts in 30ms (on my machine).</li> <li>\ud83d\udce6 Single, standalone binary\u2014no installation or dependencies required.</li> <li>\ud83e\udeb6 Lightweight (&lt; 9MB compressed) for your machine, SBCs, and servers.</li> <li>\ud83d\uddef Run many instances for simultaneous conversations.</li> <li>\ud83e\udd16 Supports AIs from OpenAI, Anthropic, DeepSeek, Google, xAI, and   llama.cpp/Ollama (local) all in a single conversation.</li> <li>\ud83d\udd76 Go incognito <code>hai -i</code>.</li> <li>\u2699 Give AI the power to run programs on your computer.</li> <li>\ud83c\udf5d Share AI prompt-pasta publicly using the task repository.</li> <li>\ud83d\udcc2 Load images, code, or text into the conversation.</li> <li>\ud83d\udd17 Load URLs with automatic article extraction and markdown conversion.</li> <li>\ud83c\udfa8 Highlights syntax for markdown and code snippets.</li> <li>\ud83d\uddbc Render output to browser.</li> <li>\ud83d\udcbe Auto-saves last conversation for easy resumption.</li> <li>\u2601 Store and share data on the cloud for easy access by AIs.</li> <li>\ud83d\udce7 Get emails from AI\u2014send notifications or share data.</li> <li>\ud83d\udee0 Open source: Apache License 2.0</li> <li>\ud83d\udcbb Supports Linux and macOS. Windows needs testing (help!).</li> </ul>"},{"location":"#video-walkthrough-youtube","title":"Video Walkthrough (YouTube)","text":""},{"location":"#more-videos","title":"More videos","text":"<ul> <li>Using hai to manage a personal calendar</li> <li>Using hai to get a code review</li> <li>Using the hai api</li> <li>Using hai to encrypt/decrypt local files as assets</li> <li>Using hai to analyze YouTube transcripts</li> <li>Using hai with a search engine</li> <li>Making the hai walkthrough with ffmpeg</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<pre><code>$ curl -LsSf https://hai.superego.ai/hai-installer.sh | sh\n$ hai\n</code></pre> <p>Other install methods</p> <p>If you're using Windows or require another method for installation, see our installation section.</p>"},{"location":"api/","title":"API","text":""},{"location":"api/#get-an-api-token","title":"Get an API token","text":"<p>You can get an API token with the <code>hai/get-api-token</code> task:</p> <pre><code>/task hai/get-api-token\n</code></pre>"},{"location":"api/#query-the-api","title":"Query the API","text":"<p>To query the API, use the <code>hai/api</code> task:</p> <pre><code>/task hai/api\n</code></pre> <p>You can use this task to ask the AI about available options or make actual requests.</p> <p>If you want to read about the details, use:</p> <pre><code>/task-view hai/api\n</code></pre>"},{"location":"config/","title":"Configuration File","text":"<p>The <code>hai</code> configuration file is stored in <code>~/.hai/hai.toml</code> where <code>~</code> is your home directory.</p>"},{"location":"config/#creation","title":"Creation","text":"<p>You won't find the configuration file until you've launched <code>hai</code> at least once.</p>"},{"location":"config/#default-ai-model","title":"Default AI model","text":"<pre><code>default_ai_model = \"gpt-4.1\"\n</code></pre> <p>The <code>/ai-default</code> command is a REPL-command that updates this key-value.</p>"},{"location":"config/#default-editor","title":"Default editor","text":"<p>Some commands require an editor, for example, editing an asset. <code>vim</code> is the default editor, but you can change it to anything that takes a file path as argument including <code>emacs</code>, <code>code</code>, <code>kate</code>, and <code>nano</code>:</p> <pre><code>default_editor = \"vim\"\n</code></pre> <p>Some editors require additional arguments to prevent forking. For example, VSCode is best configured as follows:</p> <pre><code>default_editor = \"code --new-window --disable-workspace-trust --wait\"\n</code></pre>"},{"location":"config/#default-shell","title":"Default shell","text":"<p>Programs are executed using a shell. By default, the shell is <code>bash</code> except on Windows where it's <code>powershell</code>.</p> <pre><code>default_shell = \"bash\"\n</code></pre>"},{"location":"config/#check-for-updates","title":"Check for updates","text":"<p>To disable automatic anonymous version checks when <code>hai</code> is launched, set:</p> <pre><code>check_for_updates = false\n</code></pre> <p>Pro Privacy</p> <p>By default (<code>check_for_updates = true</code>), this version check is the only outgoing request that <code>hai</code> makes automatically. All other requests occur solely as a result of explicit user actions. You can verify this with the <code>hai/code</code> task.</p>"},{"location":"config/#tool-confirmation","title":"Tool confirmation","text":"<p>You can require confirmation before executing any tool:</p> <pre><code>tool_confirm = true\n</code></pre> <p>Use this if you're worried about your AI going rogue.</p>"},{"location":"config/#temperature","title":"Temperature","text":"<p>By default, <code>temperature</code> is set to 0 across all AIs. That's hacker-friendly because it works uniformly across providers (minus some reasoning AIs) and optimizes for highest likelihood answers rather than whimsical exploration.</p> <p>To use the default temperature set by AI providers, set:</p> <pre><code>default_ai_temperature_to_absolute_zero = false\n</code></pre> <p>Alternatively, you can use the <code>/temperature</code> command in the REPL.</p>"},{"location":"cli/incognito/","title":"Incognito","text":"<p>Incognito disables local conversation history and optionally set a different default AI model.</p> <p></p>"},{"location":"cli/incognito/#start-in-incognito","title":"Start in incognito","text":"<pre><code>$ hai -i\n</code></pre>"},{"location":"cli/incognito/#setting-a-default-incognito-ai-model","title":"Setting a default incognito AI model","text":"<p>Modify <code>~/.hai/hai.toml</code>:</p> <pre><code># The default AI model in incognito mode.\n#default_incognito_ai_model = \"ollama/gpt-oss:20b\"\n</code></pre> <p>While any AI model can be specified, a local model (e.g. <code>ollama/gpt-oss:20b</code>) lets you be fully discrete: no conversation history and no data leaves your machine.</p>"},{"location":"cli/incognito/#what-history-does-hai-keep","title":"What history does <code>hai</code> keep","text":"<p>There are two logs that <code>hai</code> maintains:</p> <ul> <li>A log of recent repl-commands accessible by the up-arrow.</li> <li>The most recent conversation which is retrievable using <code>/chat-resume</code>.</li> </ul> <p>Incognito disables both of these.</p>"},{"location":"cli/run-and-exit/","title":"Run and exit","text":"<p>To run commands and exit without entering the REPL interface, use:</p> <pre><code>$ hai bye '&lt;cmd1&gt;' '&lt;cmd2&gt;' ...\n</code></pre> <p>Multiple commands can be specified. All printouts go to the terminal and <code>hai</code> exits at the end.</p> <p>Escaping</p> <p>Use single-quotes around each repl-command to minimize escaping issues.</p>"},{"location":"cli/run-and-exit/#reading-from-stdin","title":"Reading from stdin","text":"<p>Use <code>-</code> as a command to create a <code>/prep</code> message with data from stdin. For example:</p> <pre><code>cat meeting_notes.txt | hai bye - 'summary please'`\n</code></pre>"},{"location":"cli/run-and-exit/#set-model-and-user-for-consistency","title":"Set model and user for consistency","text":"<p>Use <code>-m</code> to set the model, and <code>-u</code> to set the user account:</p> <pre><code>$ hai -u &lt;username&gt; -m &lt;model&gt; bye '&lt;cmd1&gt;' '&lt;cmd2&gt;' ...\n</code></pre> <p>This eliminates variations due to the default user and model fluctuating.</p>"},{"location":"cli/run-and-exit/#non-interactive-mode","title":"Non-interactive mode","text":"<p>If running in non-interactive mode (e.g. as a cron job), use <code>-y</code> to automatically confirm all user prompts.</p>"},{"location":"cli/subcommands-and-options/","title":"Subcommands and options","text":"<p>The <code>hai</code> CLI has additional functionality that supplements the REPL.</p>"},{"location":"cli/subcommands-and-options/#specify-user","title":"Specify user","text":"<p>If you've logged into multiple <code>hai</code> user accounts, the default behavior is to use the last user account. You can force <code>hai</code> to run with a specific user account:</p> <pre><code>hai -u &lt;username&gt;\n</code></pre> <p>This requires that you've previously logged into the account with <code>/account-login</code>.</p>"},{"location":"cli/subcommands-and-options/#specify-model","title":"Specify model","text":"<p>To override the default LLM model for an invocation of <code>hai</code>, use <code>-m</code>:</p> <pre><code>hai -m &lt;model&gt;\n</code></pre>"},{"location":"cli/subcommands-and-options/#task-mode","title":"Task mode","text":"<p>To immediately drop a user into task-mode:</p> <pre><code>$ hai task &lt;task-fully-qualified-name&gt;\n</code></pre> <p>To trust a task, use <code>--trust</code>.</p>"},{"location":"cli/subcommands-and-options/#set-api-key-for-llm-provider","title":"Set API key for LLM provider","text":"<pre><code>$ hai set-key &lt;provider&gt; &lt;key&gt;\n</code></pre> <p>Supported providers: <code>openai</code>, <code>anthropic</code>, <code>google</code>, <code>deepseek</code>, <code>xai</code></p>"},{"location":"cli/websocket-queue/","title":"Websocket queue","text":"<p>Experimental</p> <p>Websocket queues are subject to breaking changes within a major version.</p> <p><code>hai</code> can expose a WebSocket interface, allowing other programs, processes, or even web services to enqueue commands for execution.</p>"},{"location":"cli/websocket-queue/#start-the-queue-listener","title":"Start the Queue Listener","text":"<p>Launch the listener with:</p> <pre><code>hai listen -a &lt;address&gt; -w &lt;whitelisted_origin_header&gt;\n</code></pre> <ul> <li>If <code>-a</code> is omitted, the listener defaults to <code>127.0.0.1:1338</code>.</li> <li>To prevent unauthorized access, use <code>-w</code> to whitelist an <code>Origin</code> header   value. Only requests with a matching <code>Origin</code> header will be accepted.</li> </ul> <p>Note</p> <p>If a request does not include an <code>Origin</code> header (it does not originate from a browser), the whitelist is not enforced and the request is allowed.</p>"},{"location":"cli/websocket-queue/#enqueue-commands-via-websocket","title":"Enqueue Commands via WebSocket","text":"<p>Connect to the WebSocket and send a JSON payload like:</p> <pre><code>{\n  \".tag\": \"push\",\n  \"cmds\": [\n    \"/load-url &lt;url-populated-by-requester&gt;\",\n    \"Key takeaways?\",\n  ]\n}\n</code></pre> <ul> <li>The <code>cmds</code> array can contain one or more commands to enqueue.</li> </ul> <p>As long as the <code>hai listen</code> process is running, incoming commands will be added to the queue.</p>"},{"location":"cli/websocket-queue/#process-the-queue","title":"Process the Queue","text":"<p>From within any REPL instance, use:</p> <pre><code>/queue-pop\n</code></pre> <p>This will run all commands from a single websocket message.</p>"},{"location":"getting-started/api-keys/","title":"API keys","text":"<p>To use AI/LLMs with <code>hai</code>, you'll need to authenticate with AI providers in one of two ways:</p>"},{"location":"getting-started/api-keys/#use-your-own-api-keys","title":"Use your own API keys","text":"<p>Set an API key for each provider (<code>openai</code>, <code>anthropic</code>, <code>google</code>, <code>deepseek</code>, <code>xai</code>) you intend to use. Choose any of the following methods:</p> <ul> <li>CLI command: <pre><code>$ hai set-key &lt;provider&gt; &lt;key&gt;\n</code></pre></li> <li>REPL command: <pre><code>/set-key &lt;provider&gt; &lt;key&gt;\n</code></pre></li> <li>Environment variable: <pre><code>$ &lt;PROVIDER&gt;_API_KEY=&lt;key&gt; hai\n</code></pre></li> <li>Config file: Add your keys to <code>~/.hai/hai.toml</code> <pre><code>[openai]\napi_key = \"&lt;key&gt;\"\n\n[anthropic]\napi_key = \"&lt;key&gt;\"\n\n[google]\napi_key = \"&lt;key&gt;\"\n\n[deepseek]\napi_key = \"&lt;key&gt;\"\n\n[xai]\napi_key = \"&lt;key&gt;\"\n</code></pre></li> </ul>"},{"location":"getting-started/api-keys/#use-hai-router","title":"Use <code>hai router</code>","text":"<p><code>hai router</code> is a subscription service that gives you access to all supported AI providers without using your own keys.</p> <ul> <li>See subscription details and subscribe:   <pre><code>/account-subscribe\n</code></pre></li> <li>Enable via REPL:   <pre><code>/hai-router on\n</code></pre></li> <li>Works on every machine once you login:   <pre><code>/account-login\n</code></pre></li> </ul> <p><code>hai router</code> is an easy way to support the hai project and its ongoing development.</p>"},{"location":"getting-started/getting-help/","title":"Getting help","text":""},{"location":"getting-started/getting-help/#help-menus","title":"Help menus","text":""},{"location":"getting-started/getting-help/#command-line-help","title":"Command-line help","text":"<p>The <code>--help</code> flag shows the options available for the <code>hai</code> program:</p> <pre><code>$ hai --help\n</code></pre>"},{"location":"getting-started/getting-help/#repl-help","title":"REPL help","text":"<p>The <code>/help</code> command shows the commands available in the <code>hai</code> REPL:</p> <pre><code>[0] /help\n</code></pre>"},{"location":"getting-started/getting-help/#help-task","title":"Help task","text":"<p>The AI can assist you conversationally. Just use the <code>hai/help</code> task to load the necessary context:</p> <pre><code>[0] /task hai/help\n</code></pre>"},{"location":"getting-started/getting-help/#viewing-the-version","title":"Viewing the version","text":"<p>When seeking help, it's important to determine the version of uv that you're using \u2014 sometimes the problem is already solved in a newer version.</p> <p>To check the installed version:</p> <pre><code>$ hai --version\n</code></pre>"},{"location":"getting-started/getting-help/#open-an-issue-on-github","title":"Open an issue on GitHub","text":"<p>You can report bugs and request features on the <code>hai</code> issue tracker on GitHub.</p>"},{"location":"getting-started/getting-help/#chat-on-discord","title":"Chat on Discord","text":"<p>Invite to Discord server</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#standalone-installer","title":"Standalone Installer","text":"<p>Linux and macOS</p> <pre><code>$ curl -LsSf https://hai.superego.ai/hai-installer.sh | sh\n</code></pre>"},{"location":"getting-started/installation/#alt-download-binary","title":"Alt: Download binary","text":"<p>Linux, macOS, and Windows</p> <p>Go to releases and download the version for your machine.</p>"},{"location":"getting-started/installation/#alt-build-from-source","title":"Alt: Build from source","text":"<p>Linux, macOS, and Windows</p> <pre><code>$ cargo install hai-cli\n</code></pre>"},{"location":"getting-started/installation/#run","title":"Run","text":"<pre><code>$ hai\n</code></pre>"},{"location":"repl/account/","title":"Account","text":"<p>Accounts are optional but are necessary for features including:</p> <ul> <li>Publishing a task</li> <li>Sharing an asset publicly</li> </ul>"},{"location":"repl/account/#view-active-account","title":"View active account","text":"<p>To check the active account and see all available accounts, use:</p> <pre><code>/account\n</code></pre>"},{"location":"repl/account/#create-a-new-account","title":"Create a new account","text":"<pre><code>/account-new\n</code></pre>"},{"location":"repl/account/#login-to-an-account","title":"Login to an account","text":"<pre><code>/account-login\n</code></pre>"},{"location":"repl/account/#switch-active-account","title":"Switch active account","text":"<pre><code>/account &lt;username&gt;\n</code></pre> <p>You must have previously logged into this account.</p>"},{"location":"repl/account/#who-is","title":"Who-is","text":"<p>To get information about a user including the tasks they've published, use:</p> <pre><code>/whois &lt;username&gt;\n</code></pre>"},{"location":"repl/account/#subscribe-to-support-the-project","title":"Subscribe to support the project","text":"<pre><code>/account-subscribe\n</code></pre> <p>Benefits include:</p> <ul> <li>Access to <code>hai router</code></li> <li>Increased asset quota</li> <li>Increased email quota</li> </ul>"},{"location":"repl/account/#see-hai-router-credit-balance","title":"See <code>hai router</code> credit balance","text":"<pre><code>/account-balance\n</code></pre>"},{"location":"repl/assets/","title":"Assets","text":"<p>Assets are data objects stored in the cloud with a filesystem-like interface. In addition to basic blob storage, there are additional conveniences for you and your LLM: syncing changed data down, listening for changes to assets, temporarily making assets available locally, and revision control.</p>"},{"location":"repl/assets/#create-an-asset","title":"Create an asset","text":"<p>To create or open an existing asset, use:</p> <pre><code>/asset &lt;name&gt;\n/a &lt;name&gt;\n</code></pre> <p>Asset names as paths</p> <p>Asset names can mimic paths by using forward slash, e.g. <code>/asset a/b/c</code></p> <p>This opens your configured editor (defaults to <code>vim</code>) to write the contents.</p> <p>To skip the editor, write a multi-line command (<code>Alt + Enter</code> or <code>Option + Enter</code>) to set the contents:</p> <pre><code>/asset &lt;name&gt;\ntest\n1 2 3\n</code></pre> <p>To use another editor:</p> <pre><code>/asset &lt;name&gt; &lt;editor-cmd&gt;\n</code></pre>"},{"location":"repl/assets/#load-an-asset","title":"Load an asset","text":"<p>To load an asset into a conversation, use:</p> <pre><code>/asset-load &lt;name&gt;\n</code></pre> <p>Tab complete for asset names</p> <p>When specifying asset names, use tab for auto-completion.</p> <p>To load an asset and print its contents in the repl, use:</p> <pre><code>/asset-view &lt;name&gt;\n</code></pre> <p>Similar local files loaded with <code>/load</code>, loaded assets are retained after a <code>/reset</code>.</p>"},{"location":"repl/assets/#temporary-local-copy-of-asset","title":"Temporary local copy of asset","text":"<p>It's undesirable to load an asset into a conversation when the data is better transformed by a function written by the LLM rather than by the LLM itself.</p> <p>Having an LLM transform data is prone to many issues: slow data processing, costly output tokens, hallucinations, exceeding the context window, and lack of support for non-unicode data.</p> <p>The solution is to make a temporary copy of an asset and have the LLM operate on the file:</p> <pre><code>/asset-temp &lt;name&gt;\n</code></pre> <p>This makes a temporary local copy of the asset and adds the file path to the conversation.</p> <p>Here's an example processing The Odyssey by Homer:</p> <p><pre><code>[0]: /asset-temp book/odyssey.txt\n</code></pre> <pre><code>Asset 'book/odyssey.txt' copied to '/var/folders/nv/ytdlylsn7kn_x53vcy4pz1xr0000gn/T/asset_RPcI7k.txt'\n</code></pre> <pre><code>[2] !sh count occurrences of ulysses\n</code></pre> <pre><code>\u2193\u2193\u2193\n\ngrep -io 'ulysses' /var/folders/nv/ytdlylsn7kn_x53vcy4pz1xr0000gn/T/asset_RPcI7k.txt | wc -l\n</code></pre> <pre><code>\u2699 \u2699 \u2699\n\n580\n</code></pre></p> <p>The temporary file is automatically cleaned up when the conversation ends.</p> <p>To make copies of the most recent <code>n</code> revisions of an asset, try:</p> <pre><code>/asset-temp &lt;name&gt; &lt;n&gt;\n</code></pre>"},{"location":"repl/assets/#syncing-assets-locally","title":"Syncing assets locally","text":"<p>To sync local copies of assets, use:</p> <pre><code>/asset-sync-down &lt;prefix&gt; &lt;path&gt;\n</code></pre> <p>This does a one-way sync of all assets with the given prefix to a local path. Asset names are reproduced on the local filesystem and forward slashes are converted to folders. Existing assets are not downloaded again unless their contents have changed.</p> <p>No information is added to the conversation history. You will need to inform the LLM of relevant files. For example, by listing the files: <code>!!ls &lt;path&gt;</code></p> <p>Any assets with metadata will have those synced as well as separate files with a <code>.metadata</code> extension.</p>"},{"location":"repl/assets/#sharing-links-to-assets","title":"Sharing links to assets","text":"<p>To generate a link to an asset that's valid for 24 hours, use:</p> <pre><code>/asset-link &lt;name&gt;\n</code></pre>"},{"location":"repl/assets/#iterating-through-revisions","title":"Iterating through revisions","text":"<p>To see revisions of an asset, use:</p> <pre><code>/asset-revisions &lt;name&gt;\n</code></pre> <p>This is interactive and requires input to jump to each revision. To simply dump <code>count</code> revisions to the conversation, use:</p> <pre><code>/asset-revisions &lt;name&gt; &lt;count&gt;\n</code></pre>"},{"location":"repl/assets/#import-or-export-assets","title":"Import or export assets","text":"<p>To import an asset from a local file:</p> <pre><code>/asset-import &lt;name&gt; &lt;path&gt;\n</code></pre> <p>To export an asset to a local file:</p> <pre><code>/asset-export &lt;name&gt; &lt;path&gt;\n</code></pre>"},{"location":"repl/assets/#listing-assets","title":"Listing assets","text":"<p>To list assets, use:</p> <pre><code>/asset-list [&lt;prefix&gt;]\n/ls [&lt;prefix&gt;]\n</code></pre> <p>Specifying a <code>prefix</code> filters the result set. The <code>prefix</code> can be arbitrary and does not need to be aligned with a folder segment.</p>"},{"location":"repl/assets/#public-assets","title":"Public assets","text":"<p>Public assets start with a forward slash followed by a username (<code>/&lt;username&gt;</code>):</p> <p>For example, substituting your username you can create a public file:</p> <pre><code>/asset /&lt;username&gt;/public.txt\n</code></pre> <p>Any other user can view it with:</p> <pre><code>/asset-view /&lt;username&gt;/public.txt\n</code></pre> <p>Public assets shortcut</p> <p>You can also use <code>//</code> as a shortcut to refer to your own public asset path. For example, <code>/asset //public.txt</code> is equivalent to <code>/asset /&lt;username&gt;/public.txt</code>. This makes it easier to access your public assets and lets you write task steps that are generic to the logged-in account.</p>"},{"location":"repl/assets/#search","title":"Search","text":"<p>Assets can be searched semantically based on their contents:</p> <pre><code>/asset-search cook salmon\n</code></pre> <p>The search is powered by embeddings on the content and the <code>title</code> metadata key if it's set. The latter is especially important if the content is non-unicode.</p>"},{"location":"repl/assets/#usage-with-exec","title":"Usage with <code>/exec</code>","text":"<p>When executing a shell command, use <code>@name</code> to reference an asset. The asset will be transparently downloaded into a temporary file.</p> <p><pre><code>[0] !!cat @/hai/changelog | grep -A 2 v1.3.0\nequivalent to:\n[0] !!grep -A 2 v1.3.0 @/hai/changelog\n</code></pre> <pre><code>## v1.3.0\n\n- Add syntax highlighting for code blocks.\n</code></pre></p> <p>Note: <code>!!</code> is shorthand for <code>/exec</code>.</p> <p>If a shell redirects (<code>&gt;</code> or <code>&gt;&gt;</code>) to an @asset, the output file will be uploaded as well.</p> <pre><code>[0] !!grep -A 2 v1.3.0 @/hai/changelog &gt; @changes-v1.3.0\n</code></pre> <p>This processes a public asset from the <code>hai</code> account and saves a filtered version to the <code>changes-v1.3.0</code> private asset.</p> <p>Limitations</p> <p>The implementation uses simple string substitution to replace <code>@asset</code> markers with temporary files. Complex shell operations involving quotes or escapes around asset references may not work as expected.</p>"},{"location":"repl/assets/#write-conflicts","title":"Write conflicts","text":"<p>When the same asset is modified simultaneously by two separate <code>hai</code> processes, a write conflict occurs. The version that loses the race will be preserved as a new asset with the same name as the original but with a random suffix.</p> <p>An easy way to see the difference is to:</p> <pre><code>!!diff @file @file(suffix)\n</code></pre>"},{"location":"repl/assets/#metadata","title":"Metadata","text":"<p>Each asset can have a JSON object associated with it to store metadata:</p> Command Description <code>/asset-md-get &lt;name&gt;</code> Fetches metadata for an asset and adds it to the conversation. <code>/asset-md-set &lt;name&gt; &lt;json&gt;</code> Sets the entire metadata blob. <code>/asset-md-set-key &lt;name&gt; &lt;key&gt; &lt;value&gt;</code> Sets/replaces a metadata key. <code>/asset-md-del-key &lt;name&gt; &lt;key&gt;</code> Deletes a metadata key. <p>If the <code>title</code> metadata key is set, it's shown in <code>/asset-list</code> and <code>/asset-search</code> in <code>[]</code> brackets.</p> <p>\ud83d\ude4b Help Wanted</p> <p>Interested in using metadata to make asset encryption the default way of life? All ideas welcome. Please reach out or open an issue.</p>"},{"location":"repl/assets/#asset-push-acl","title":"Asset Push &amp; ACL","text":"<p>Your public assets (prefixed by your username <code>/username/...</code>) can have ACLs set so that an asset can be used as a write-only \"asset/document drop\".</p> <pre><code>/asset-acl /ken/hai-feedback deny:read-data\n/asset-acl /ken/hai-feedback allow:push-data\n</code></pre> <p>With these ACLs, any user can push data (<code>/asset-push</code>) into the <code>/ken/hai-feedback</code> asset, but no one except the owner can read what's been pushed.</p> <p>The owner (user <code>ken</code> in this example) can read the contents of <code>/ken/hai-feedback</code> using <code>/asset-list-revisions</code> and can access revisions with <code>/asset-get-revision</code>.</p>"},{"location":"repl/assets/#listening-for-changes","title":"Listening for changes","text":"<p><code>/asset-listen &lt;name&gt;</code> can be used to block the REPL until a change to the asset. You can test this by:</p> <pre><code># Console 1 (create asset)\n/asset test-listen\n\n# Console 2 (listen for changes -- blocking)\n/asset-listen test-listen\n\n# Console 1 (modify, unblocking console 2)\n/asset test-listen\n</code></pre> <p>For an example of sending emails based on asset changes, see the hai/email-asset-updates task: <code>/task hai/email-asset-updates</code></p> <p>Note that the API exposes a websockets interface that pushes notifications when changes occur.</p>"},{"location":"repl/assets/#collapsing-folders","title":"Collapsing Folders","text":"<p>The underlying asset store uses a key-value structure. While asset keys may contain forward slashes to resemble directory paths, these are purely cosmetic.</p> <p>By default, when listing your assets, they appear as a flat list:</p> <p><pre><code>[0] /ls\n</code></pre> <pre><code>a/b/c\na/b/d\na/b/e/f\n</code></pre></p> <p>To make browsing easier, you can choose to \"collapse\" specific folders:</p> <ul> <li><code>/asset-folder-collapse &lt;path&gt;</code> - Collapse a folder when listing a parent prefix.</li> <li><code>/asset-folder-expand &lt;path&gt;</code> - Uncollapse a folder when listing a parent prefix.   Note that all folders are expanded by default.</li> <li><code>/asset-folder-list [&lt;prefix&gt;]</code> - List all collapsed folders with the given   path prefix.</li> </ul> <p>Collapsing the <code>a/b</code> folder:</p> <pre><code>[1] /asset-collapse a/b\n</code></pre> <p>Now, listing the root shows the collapsed folder:</p> <p><pre><code>[2] /ls\n</code></pre> <pre><code>a/b\ud83d\udcc1\n</code></pre></p> <p>To view the contents of the collapsed folder, list it directly:</p> <p><pre><code>[3] /ls a/b/\n</code></pre> <pre><code>a/b/c\na/b/d\na/b/e/f\n</code></pre></p> <p>There's a limit of 100 collapsed folders each for your private and public assets.</p>"},{"location":"repl/assets/#quota","title":"Quota","text":"<p>Each account gets 1GB of asset storage.</p> <p>Revisions</p> <p>The size of each revision is counted towards your storage. When an asset is removed, the total size of all of its revisions is removed from your quota.</p> <p>To increase your account's storage quota, see <code>/account-subscribe</code>.</p>"},{"location":"repl/basics/","title":"REPL basics","text":""},{"location":"repl/basics/#launch","title":"Launch","text":"<pre><code>$ hai\n</code></pre> <p>To exit, use <code>/quit</code> or <code>Ctrl + D</code>.</p>"},{"location":"repl/basics/#chat","title":"Chat","text":"<p>Any message that isn't a <code>/command</code> or <code>!tool</code> is a chat message that triggers a chat response from the LLM.</p> <p><pre><code>[0] hi\n</code></pre> <pre><code>\u2193\u2193\u2193\n\nHello! How can I help you today?\n</code></pre></p>"},{"location":"repl/basics/#repl-prompt","title":"REPL Prompt","text":"<p>On the left-hand side is the index of the message in the conversation. For example, the first message is always <code>[0]</code> or <code>[1]</code> if a system-prompt is set.</p> <p>On the right-hand side is a set of status information:</p> <ul> <li>The current working directory which is important for tools that use the local   filesystem.</li> <li>The token count of the conversation. See Cost estimation.</li> <li>The active LLM model.</li> <li>The current timestamp.</li> </ul>"},{"location":"repl/basics/#new-chat","title":"New Chat","text":"<p>To start a new conversation:</p> <pre><code>/new\n/n -- shortcut\n</code></pre> <p>This removes all messages unless the REPL is in task mode.</p>"},{"location":"repl/basics/#reset-chat","title":"Reset Chat","text":"<p>Resetting a conversation removes all conversational messages but keeps loaded data (<code>/load</code>, <code>/load-url</code>, <code>/asset-load</code>):</p> <pre><code>/reset\n/r -- shortcut\n</code></pre>"},{"location":"repl/basics/#load-a-file","title":"Load a file","text":"<p>To load a file into the chat conversation, use:</p> <pre><code>/load &lt;path&gt;\n</code></pre> <p>To be vigilant about token usage, check the token count on the status line.</p>"},{"location":"repl/basics/#load-a-url","title":"Load a URL","text":"<p>To load a URL into the chat conversation, use:</p> <pre><code>/load-url &lt;url&gt;\n</code></pre> <p>PNG and JPG responses will be rendered in the terminal and can be used with image-capable LLM models.</p> <p>HTML responses are automatically converted to markdown for significantly improved token efficiency. If this is undesirable, use:</p> <pre><code>/load-url(raw) &lt;url&gt;\n</code></pre>"},{"location":"repl/basics/#execute-program-and-load-stdoutstderr","title":"Execute program and load stdout/stderr","text":"<p>To execute a program, use:</p> <pre><code>/exec &lt;prog&gt;\n</code></pre> <p>Both the standard output and standard error will be added to the chat conversation. If the status code is non-zero, it is added to the conversation as well.</p> <p>The LLM is not prompted for a response until your next message.</p> <p>The shortcut for execution is double-exclamation:</p> <pre><code>!!&lt;prog&gt;\n</code></pre> <p>Programs are executed using <code>bash</code> or <code>powershell</code> on Windows but can be changed in default shell.</p>"},{"location":"repl/basics/#example","title":"Example","text":"<pre><code>[0]: /exec ls ~/Documents\n\nOR\n\n[0]: !!ls ~/Documents\n</code></pre> <pre><code>fetch_and_analyze_tweets.py\npsql_index_notes.txt\n...\n</code></pre> <pre><code>[2]: what do my docs say about me?\n</code></pre> <pre><code>\u2193\u2193\u2193\n\nYou engage in technical, organizational, and analytical work. You handle SQL, PostgreSQL, and security topics.\n</code></pre>"},{"location":"repl/basics/#change-working-directory","title":"Change working directory","text":"<p>To change your current working directory, use:</p> <pre><code>/cd &lt;path&gt;\n</code></pre> <p>Changing the working directory is important for loading files, executing programs, and using tools that interact with your local filesystem.</p>"},{"location":"repl/basics/#switch-llm-model","title":"Switch LLM model","text":"<p>To switch LLM models, use:</p> <pre><code>/ai &lt;model&gt;\n</code></pre> <p>For information on available models, see LLM models.</p>"},{"location":"repl/basics/#forget-a-message","title":"Forget a message","text":"<p>If you make a mistake, you can forget the last interaction with:</p> <pre><code>/forget [&lt;n&gt;]\n</code></pre> <p>By default, one interaction is removed from the conversation, but <code>n</code> can be specified to forget multiple interactions at once.</p> <p>For the opposite effect\u2014retaining interactions instead of removing them\u2014see <code>/keep</code>.</p>"},{"location":"repl/basics/#tab-completion","title":"Tab completion","text":"<p><code>hai</code> supports tab completion for commands and some arguments. When tab completion is unavailable, it falls back to local file path completion.</p>"},{"location":"repl/basics/#command-history","title":"Command history","text":"<p>Command history is stored in <code>~/.hai/history</code>. To access it in the REPL:</p> <ul> <li>Arrow up/down cycles through recently used commands.</li> <li><code>Ctrl + R</code> performs a reverse-search with partial string matching.</li> </ul>"},{"location":"repl/basics/#save-and-resume-chats","title":"Save and resume chats","text":"<p>Resume your last chat with:</p> <pre><code>/chat-resume\n</code></pre> <p>When is a chat saved?</p> <p>Your last chat is saved locally whenever you exit <code>hai</code> or start a new conversation with <code>/new</code> or <code>/reset</code>.</p> <p>To save a chat for the long term as an asset:</p> <pre><code>/chat-save\n</code></pre> <p>By default, chats are saved as assets named <code>chat/&lt;timestamp&gt;</code>. A descriptive title is automatically generated and stored in the asset metadata for easier discovery. For example:</p> <pre><code>[0] /ls\nchat/2025-04-08-203003 [Public/Private Key Management for Encryption and Signing]\n</code></pre> <p>Resume a saved chat:</p> <pre><code>/chat-resume &lt;name&gt;\n</code></pre> <p>Save with a custom name:</p> <pre><code>/chat-save [&lt;name&gt;]\n</code></pre>"},{"location":"repl/basics/#cost-estimation","title":"Cost estimation","text":"<p>The REPL's right-hand prompt shows the number of tokens in the current conversation. To understand how much the converation has cost so far and what the input cost is for the next prompt, use:</p> <pre><code>/cost\n</code></pre> <p>This is especially useful when files, urls, images, and assets have been loaded for context.</p> <p>Tokenizer</p> <p>Token counts are estimated using the GPT-3.5 tokenizer because of its smaller size and therefore faster loading time. Unscientifically, token counts can differ by as much as 20%.</p> <p>Tokens for images</p> <p>Token counts for images are only accurate for OpenAI models because all images are assumed to consume the hard coded number of tokens for a \"low detail\" image in the OpenAI API.</p>"},{"location":"repl/basics/#pin-and-prep-messages","title":"Pin and prep messages","text":"<p>To add a message that doesn't prompt the LLM and is retained on <code>/reset</code>, use:</p> <pre><code>/pin &lt;message&gt;\n</code></pre> <p>To add a message that doesn't prompt the LLM but is cleared on <code>/reset</code>, use:</p> <pre><code>/prep &lt;message&gt;\n</code></pre> <p>Using <code>/prep</code> directly is uncommon. A shortcut for <code>/prep</code> is to add two blank newlines at the end of a message. This is helpful if you want to add more content (such as a copy-and-paste) before requesting a response.</p>"},{"location":"repl/basics/#system-prompt","title":"System prompt","text":"<p>To set a system prompt, use:</p> <pre><code>/system-prompt &lt;message&gt;\n</code></pre>"},{"location":"repl/basics/#copy-to-clipboard","title":"Copy to clipboard","text":"<p>To copy the last message verbatim to your machine's clipboard, use:</p> <pre><code>/clip\n</code></pre> <p>For a version that lets you prompt the LLM to add to your clipboard, see !clip.</p>"},{"location":"repl/llm-models/","title":"LLM models","text":"<p><code>hai</code> supports models from OpenAI, Anthropic, DeepSeek, Google, and xAI. Local models served via llama.cpp or Ollama are supported as well.</p> <p></p>"},{"location":"repl/llm-models/#default-model","title":"Default model","text":"<p>The default model is the active model when a new instance of <code>hai</code> is launched.</p> <p>Select a default model with:</p> <pre><code>/ai-default &lt;model&gt;\n</code></pre> <p>This writes to your <code>hai.toml</code> config which can also be changed manually:</p> <pre><code>default_ai_model = \"gpt-5\"\n</code></pre> <p>If a default model isn't set, it's automatically chosen based on API key availability.</p>"},{"location":"repl/llm-models/#switching-models","title":"Switching models","text":"<p>Switching is as easy as:</p> <pre><code>/ai &lt;model&gt;\n</code></pre> <p>Models can even be switched mid-conversation as long as the target model supports all the capabilities (images, tools) utilized in the conversation.</p>"},{"location":"repl/llm-models/#notable-models","title":"Notable models","text":"Provider Notable Models (Not Comprehensive) OpenAI gpt-5 (<code>g5</code>), gpt-5-mini (<code>g5m</code>), gpt-5-nano (<code>g5n</code>), gpt-4.1 (<code>41</code>), gpt-4.1-mini (<code>41m</code>), gpt-4.1-nano (<code>41n</code>), chatgpt-4o, gpt-4o (<code>4o</code>), gpt-4o-mini (<code>4om</code>) o4-mini (<code>o4m</code>), o3, o3-mini (<code>o3m</code>), o1, o1-mini (<code>o1m</code>) Anthropic sonnet-4 (<code>sonnet</code>), sonnet-4-thinking (<code>sonnet-thinking</code>), opus-4 (<code>opus</code>), opus-4-thinking (<code>opus-thinking</code>), haiku-3.5 (<code>haiku</code>) Google gemini-2.5-flash (<code>flash25</code>), gemini-2.5-pro (<code>gemini25pro</code>), gemini-2.0-flash (<code>flash20</code>) DeepSeek deepseek-reasoner (<code>r1</code>), deepseek-chat (<code>v3</code>) xAI grok-4 Ollama gemma3:27b, gpt-oss:20b, llama3.2, llama3.2-vision <p>If a model doesn\u2019t have a built-in shortcut, or if you want to use a specific version, you can specify it as <code>&lt;ai_provider&gt;/&lt;official_model_name&gt;</code>.</p>"},{"location":"repl/llm-models/#llamacpp-server","title":"<code>llama.cpp</code> server","text":"<p><code>llama-server</code> serves only one model per instance decided by the command-line arguments. Because the <code>model</code> parameter in the API request is ignored, <code>llamacpp</code> can be specified as the <code>&lt;ai_provider&gt;</code> without any model name.</p> <pre><code>/ai llamacpp\n</code></pre>"},{"location":"repl/send-email/","title":"Send email","text":""},{"location":"repl/send-email/#add-and-verify-email","title":"Add and verify email","text":"<p>Before you can send emails with <code>hai</code>, you'll need to add and and verify an email. Do this with the <code>hai/add-email</code> task:</p> <pre><code>/task hai/add-email\n</code></pre> <p>The AI will prompt you for an email and then verification code:</p> <pre><code>[0] /task hai/add-email\n...\n[5] add x@y.com as email recipient\n[9] verify code 'xyzabc'\n</code></pre> <p>For now, you can only add one registered email and the intent is for it to be your primary email account.</p>"},{"location":"repl/send-email/#send-command","title":"Send command","text":"<p>To send an email, use <code>/email</code> with a multi-line input (<code>Alt + Enter</code> or <code>Option + Enter</code>):</p> <pre><code>/email &lt;subject&gt;\n&lt;body&gt;\n</code></pre> <p><code>/email</code> sends an email to a default address you've verified. Use the <code>hai/add-email</code> task to configure it:</p> <pre><code>[0] /task hai/add-email\n...\n[1] add x@y.com as an email recipient\n[2] verify it with code 'xyzabc'  # from email\n</code></pre> <p>To have the AI send you an email, you'll need to use the <code>!hai</code> tool:</p>"},{"location":"repl/send-email/#how-to-have-the-llm-send-an-email","title":"How to have the LLM send an email","text":"<p>To have the LLM send an email, use the !hai tool which lets the LLM write repl commands:</p> <p><pre><code>[0] !hai send me an email with an uplifting quote of the day\n</code></pre> <pre><code>\u2193\u2193\u2193\n\n/email Uplifting Quote of the Dayay\nHi there!\n\nHere\u2019s your uplifting quote for today:\n\n\"The only way to do great work is to love what you do.\" \u2013 Steve Jobs\n\nWishing you a wonderful and inspiring day ahead!\n\nBest regards,\nYour AI Assistant\n</code></pre> <pre><code>\u2699 \u2699 \u2699\n\nPushed 1 command(s) into queue\n</code></pre></p>"},{"location":"repl/send-email/#quota","title":"Quota","text":"<p>Each account gets 100 emails per month.</p> <p>To increase your account's email quota, see <code>/account-subscribe</code>.</p>"},{"location":"repl/stdlib/","title":"Standard-Library Functions","text":"<p>These functions serve as lightweight commands without crowding the <code>/&lt;command&gt;</code> namespace.</p> Function Description <code>/std now</code> Displays the current date and time. <code>/std new-day-alert</code> Makes AI aware when a new day begins since the last interaction. <code>/std which &lt;prog&gt;</code> Checks if a specific program is available. <p>Using <code>/std now</code> is preferable to <code>/exec date</code>, as the latter requires user confirmation to execute in untrusted tasks. The same reasoning applies to <code>/std which</code> instead of <code>/exec which</code>.</p> <p>The <code>/std new-day-alert</code> function is essential for ongoing, multi-day conversations (e.g., calendar tasks). It ensures the AI is aware of a day change so it can handle relative dates accurately.</p>"},{"location":"repl/tasks/","title":"Tasks","text":"<p>A task in hai is a prompt-on-steroids that can be:</p> <ol> <li>Published publicly: <code>/task-publish &lt;path&gt;</code></li> <li>Executed by anyone using the task repo: <code>/task &lt;username&gt;/&lt;task_name&gt;</code></li> <li>Or, executed from a file: <code>/task &lt;path&gt;</code> (must start with <code>./</code>, <code>/</code>, or <code>~</code>)</li> </ol> <p>A task is made up of steps: a sequence of repl-commands. The commands are the same as the ones you use. A step can:</p> <ul> <li>Provide context</li> <li>Load resources (file, image, asset, URL)</li> <li>Execute local programs</li> <li>Prompt the user with a question</li> <li>Prompt the AI</li> <li>Cache program executions, functions, prompt responses, and answers-by-users</li> </ul> <p>Tasks make sharing workflows easy and improve their reproducibility given the non-deterministic nature of LLMs.</p>"},{"location":"repl/tasks/#use-a-task-from-the-repository","title":"Use a task from the repository","text":"<p>The command to launch a task is as follows:</p> <pre><code>/task &lt;task-fully-qualified-name&gt;\n</code></pre> <p>Every task is published under a user account. The fully-qualified name follows the following format: <code>&lt;username&gt;/&lt;task-name&gt;</code></p> <p>As an example, try:</p> <pre><code>/task ken/absolute-mode\n</code></pre> <p>This runs the <code>absolute-mode</code> task published by the user <code>ken</code>.</p> <p>It's a simple task that does only one thing: adds a system-prompt that removes all bedside manner from the LLM. You can view the task here.</p> <p>Alternatively, you can view the task with:</p> <pre><code>/task-view ken/absolute-mode\n</code></pre>"},{"location":"repl/tasks/#task-mode","title":"Task mode","text":"<p>When running a task, the repl enters task-mode. You can see this in the repl's left-hand prompt which includes the task's fully-qualified name:</p> <pre><code>ken/absolute-mode[1]:\n</code></pre> <p>In task mode, <code>/new</code> (<code>/n</code>) resets the conversation to the initial task state.</p> <p>To exit task mode, use <code>/task-end</code> or <code>Ctrl + D</code>.</p>"},{"location":"repl/tasks/#examples","title":"Examples","text":"<p>Here are some interesting tasks:</p> <ul> <li><code>ken/pelican-bicycle</code> -   simonw's pelicans on a bicycle   test</li> <li><code>hai/help</code> - Get help using hai. Ask   what's possible and how to do things.</li> <li><code>hai/api</code> - Use or learn about hai's   API.</li> <li><code>hai/get-api-token</code> -     Get an API token.</li> <li><code>hai/code</code> - Ask the AI about   hai's source code.</li> <li><code>hai/email-asset-updates</code> -   Get emails every time an asset is updated.</li> <li><code>hai/add-email</code> - Verify your     email address.</li> <li><code>hai/keypair-setup</code> -   Setup an RSA public &amp; private key pair in your assets. The public key is made   available to other users via your public asset pool   (<code>/&lt;username&gt;/pubkey/public_rsa.pem</code>) which they can use to encrypt messages   to you.</li> <li><code>ken/asset-crypt</code> -   Encrypt and upload files from your machine to your assets. Or, download and   decrypt assets you've encrypted with this task. Requires that the user has   setup an RSA keypair with the   <code>hai/keypair-setup</code> task.</li> <li><code>ken/weather</code> - Get the weekly   weather forecast.</li> <li><code>ken/absolute-mode</code> - Chat   with an AI lacking all bedside manner.</li> <li><code>ken/baby-play</code> - Based on your   baby's age, gives age-appropriate ideas for activities.</li> <li><code>ken/flashcard-add</code> - Helps   you generate and save flashcards based on the current conversation.</li> <li>Saves your flashcards as an asset: <code>flaschard/deck</code></li> <li><code>ken/flashcard-review</code> -     Review random flashcards</li> <li><code>ken/music-player</code> - Plays   random MP3s from your <code>music/*.mp3</code> assets. If lyrics are available in the   file\u2019s <code>lrc</code> metadata, it can display them line-by-line as the song plays.</li> <li><code>ken/youtube</code> - Get the transcript   of a YouTube video using <code>yt-dlp</code>.</li> <li><code>ken/pure-md-search</code> - Add   search results in markdown to your conversation. Needs API token (free tier   available) from pure.md.   [Video]</li> <li><code>ken/code-review</code> - Get a   code review of unstaged/staged/committed changes in your local git repo.</li> <li><code>hai/quick-task</code> - Ask AI to   help you write a task.</li> <li><code>ken/task-safety-checker</code> -   Check that a task in the hai task repo isn't obviously destructive.</li> <li><code>ken/calendar</code> -   Manage your personal calendar using plain text assets.</li> <li><code>ken/cargo-build-fix</code> -   Tries to patch rust code to fix <code>cargo build</code> errors automatically.</li> </ul>"},{"location":"repl/tasks/#searching-for-a-task","title":"Searching for a task","text":"<p>You can find tasks that other users have published using:</p> <pre><code>/task-search &lt;query&gt;\n</code></pre> <p>To see tasks published by a specific user, use:</p> <pre><code>/whois &lt;username&gt;\n</code></pre>"},{"location":"repl/tasks/#updating-a-task","title":"Updating a task","text":"<p>When a task is run, it's cached on your machine for folllow up invocations. To replace your cached copy with the latest version of a task, use:</p> <pre><code>/task-update &lt;task-fully-qualified-name&gt;\n</code></pre>"},{"location":"repl/tasks/#run-a-specific-version","title":"Run a specific version","text":"<p>List all versions of a task:</p> <pre><code>/task-versions &lt;task-fully-qualified-name&gt;\n</code></pre> <p>Run a specific version:</p> <pre><code>/task &lt;task-fully-qualified-name&gt;@&lt;version&gt;\n</code></pre>"},{"location":"repl/tasks/#task-cache","title":"Task cache","text":"<p>Tasks can cache some of their steps: <code>/ask-human(cache)</code>, <code>/exec(cache)</code>, and <code>/prompt(cache)</code>. To purge this cache, try:</p> <pre><code>/task-purge &lt;task-fully-qualified-name&gt;\n</code></pre> <p>To invoke a task with a non-default cache bucket, use the <code>key</code> option:</p> <pre><code>/task(key=\"&lt;cache_bucket&gt;\") &lt;task-fully-qualified-name&gt;\n</code></pre> <p>This lets you run a single task with a different configuration per key.</p>"},{"location":"repl/tasks/#trusting-a-task","title":"Trusting a task","text":"<p>Some task steps require user confirmation because of the danger they pose (see Security Warning). To skip these confirmations, you can set the <code>trust</code> option: <code>/task(trust)</code></p>"},{"location":"repl/tasks/#writing-a-task","title":"Writing a task","text":"<p>Tasks are written in toml with a set of required fields:</p> <pre><code># Replace &lt;username&gt; with your own\nname = \"&lt;username&gt;/demo-task\"\n\n# Semantic version\nversion = \"1.0.0\"\n\n# Displayed in /task-search\ndescription = \"A demo task to learn how to write them.\"\n\n# Uncomment to require a specific version of hai\n#dependencies = [\n#    \"hai &gt;= 1.19.0\"\n#]\n\n# Uncomment to hide this task from your /whois profile and search\n# unlisted = true\n\n# List of repl-commands to execute to setup the task\nsteps = [\n    \"/system-prompt you're sherlock holmes\",\n    \"!!ls ~/Documents\",\n    \"/prompt based on the documents i keep, which star trek character am i?\",\n]\n</code></pre> <p>Replace <code>&lt;username&gt;</code> and save this to a file called <code>demo-task.toml</code>. You can now run the task from this local file:</p> <pre><code>/task ./demo-task.toml\n</code></pre> <p>Tip</p> <p>To avoid ambiguity with tasks in the repo, when specifying a local file the file path must begin with <code>./</code>, <code>/</code>, or <code>~</code>.</p>"},{"location":"repl/tasks/#task-toml-reference","title":"Task TOML Reference","text":"Field Description <code>name</code> This must be your username followed by the name of your task. All tasks are namespaced by a username to avoid duplicates and confusion. <code>version</code> Must be a semantic version (semver). <code>description</code> Explain what the task is for. Helps for task search. <code>dependencies</code> Require the <code>hai</code> client to satisfy a semver. Useful if the task uses a command that only became available after a certain version. <code>unlisted</code> Hides the task from search and your /whois profile. <code>steps</code> Every step is something you could have typed yourself into the CLI. At the conclusion of the steps, the user takes over with the context fully populated."},{"location":"repl/tasks/#publish-a-task","title":"Publish a task","text":"<p>When your task is ready to publish, run:</p> <pre><code>/task-publish ./path/to/demo-task.toml\n</code></pre> <p>The version must be greater than the latest currently in the repo.</p> <p>Anyone can run your task by using its fully-qualified name:</p> <pre><code>/task &lt;username&gt;/demo-task\n</code></pre>"},{"location":"repl/tasks/#using-a-task-to-make-a-task","title":"Using a task to make a task","text":"<p>To have the AI help you write a task, use:</p> <pre><code>/task hai/quick-task\n</code></pre> <p>You can discuss with the AI what you want the task to accomplish. When you're done, save the task definition to a toml file and <code>/task-publish</code> it.</p> <p>If you already have an active conversation with loaded resources (files, URLs, or assets), you can ask the AI to use the current context as the basis for your new task. This is especially useful for creating reusable tasks that automatically load your commonly-used resources.</p>"},{"location":"repl/tasks/#task-specific-commands","title":"Task-specific commands","text":"<p>There are some <code>hai</code>-repl commands that are specifically made for tasks:</p> Command Description <code>/ask-human &lt;prompt&gt;</code> Ask the user a question. <code>/ask-human(secret) &lt;prompt&gt;</code> Ask a question; user's answer is treated as a secret and hidden. <code>/ask-human(cache) &lt;prompt&gt;</code> Ask a question; previous answer is reused on rerun. Use <code>/task-forget</code> to reset. <code>/set-mask-secrets on</code> Mask AI output that includes secrets in the terminal. Useful for hiding sensitive info like API tokens. <code>/exec &lt;cmd&gt;</code> Execute a command on the local machine. Always prompts the user for confirmation. <code>/exec(cache) &lt;cmd&gt;</code> Execute a command; output is cached and reused on rerun. <code>/prompt &lt;message&gt;</code> Explicitly prompt the AI with a message. <code>/prompt(cache) &lt;message&gt;</code> Prompt the AI; cached output is reused on rerun to save time and cost. <code>/task-include &lt;name|path&gt;</code> Run task steps without entering or exiting task-mode. Useful for including tasks within other tasks. <code>/ai &lt;model&gt;</code> Set the AI model. In tasks, if the model isn't available, the current model remains unchanged. <code>/keep &lt;bottom&gt; [&lt;top&gt;]</code> Forget messages to bound conversation size; useful for looping tasks. <code>/pin(&lt;accent&gt;) &lt;message&gt;</code> Add a message for the AI or user without prompting the AI for a response. Accent can be <code>danger</code>, <code>warn</code>, <code>info</code>, or <code>success</code>."},{"location":"repl/tasks/#security-warning","title":"Security warning","text":"<p>The primary attack vector to defend against is a published task that's crafted to delete or exfiltrate your data. Be careful when running any task. All potentially dangerous commands require a \"yes/no\" confirmation.</p> <p>Specifically, tasks may <code>/exec</code> commands on your machine which can both delete and exfiltrate data (e.g. make an http request). Tasks may <code>/load</code> data that can then be exfiltrated. Tasks may use a tool (e.g. <code>!sh</code> or <code>!py</code>) which can delete and exfiltrate. Tasks may use the <code>!hai</code> tool which may generate a list of commands that can delete and exfiltrate.</p>"},{"location":"repl/tools/","title":"Tools","text":"<p>Tools let the AI do more than just respond with a message.</p> <p></p>"},{"location":"repl/tools/#shell-tool-sh","title":"Shell tool <code>!sh</code>","text":"<p><code>!sh &lt;prompt&gt;</code> - Ask the AI to execute shell commands directly.</p> <p>The shell tool prompts the AI to generate a shell script to be executed on your machine. Both the code and the output is added to the conversation history.</p> <pre><code>[0]: !sh list my home dir\n</code></pre> <pre><code>\u2193\u2193\u2193\n\nls -lh ~\n\n\u2699 \u2699 \u2699\n\nDesktop\nDocuments\n...\n</code></pre>"},{"location":"repl/tools/#require-confirmation","title":"Require confirmation <code>!?</code>","text":"<p>If you're worried about destructive side effects, you can require your final confirmation with <code>!?sh</code>.</p> <pre><code>[0]: !?sh delete evidence.txt\n</code></pre> <pre><code>\u2193\u2193\u2193\n\nrm evidence.txt\n\n\u2699 \u2699 \u2699\n\n[QUESTION] Execute? y/[n]:\n</code></pre>"},{"location":"repl/tools/#make-tool-optional","title":"Make tool optional <code>?</code>","text":"<p>To let the AI to decide whether to use your suggested tool, add a \"?\" as a suffix to the tool\u2019s name.</p> <pre><code>[0]: !sh? how old is the earth?\n</code></pre> <pre><code>The Earth is approximately 4.54 billion years old.\n</code></pre> <p>Note that the answer was given without invoking the shell tool.</p>"},{"location":"repl/tools/#repeat","title":"Repeat <code>!</code>","text":"<p>Sometimes simply asking the AI the same prompt over again helps it work through errors it may have encountered in script or code writing. A lone <code>!</code> in the REPL is a shortcut to do this. Note that the previous invocation and error is now in the conversation history which guides the AI to improve its answer.</p> <p><pre><code>[0]: !sh what's the weather in santa clara, ca\n</code></pre> <pre><code>\u2193\u2193\u2193\n\ncurl -s 'wttr.in/Santa+Clara,CA?format=%C+%t'\n\n\u2699 \u2699 \u2699\n\nUnknown location; please try ~37.2333253,-121.6846349\n</code></pre> <pre><code>[3]: !\n</code></pre> <pre><code>\u2193\u2193\u2193\n\ncurl -s 'wttr.in/Santa+Clara?format=%C+%t'\n\n\u2699 \u2699 \u2699\n\nPartly cloudy +51\u00b0F\n</code></pre></p>"},{"location":"repl/tools/#same-tool-different-prompt-prompt","title":"Same tool, different prompt <code>! &lt;prompt&gt;</code>","text":"<p>If you need to change your prompt while using the same tool, use <code>! &lt;prompt&gt;</code> (note that <code>sh</code> is omitted). Following from the above weather example:</p> <p><pre><code>[6]: ! how about tokyo?\n</code></pre> <pre><code>\u2193\u2193\u2193\n\ncurl -s 'wttr.in/Tokyo?format=%C+%t'\n\n\u2699 \u2699 \u2699\n\nClear +59\u00b0F\n</code></pre></p>"},{"location":"repl/tools/#tool-mode","title":"Tool mode","text":"<p>If you find yourself using the same tool over-and-over, you can enter tool-mode by specifying a tool without a prompt (e.g. <code>!sh</code>).</p> <p><pre><code>[0]: !sh\n</code></pre> <pre><code>Entering tool mode; All messages are treated as prompts for !sh.\n</code></pre></p> <p>To exit tool mode, use <code>!exit</code> or <code>Ctrl + D</code>.</p> <p>Tool mode in tasks</p> <p>When publishing tasks, you can place users directly into tool-mode by making it the final command in your task's list of steps. This approach is helpful when your task relies on a tool to respond to user requests. If your task requires tool use and regular text responses, consider using the optional suffix for the tool mode (e.g. <code>!sh?</code>).</p>"},{"location":"repl/tools/#python-tool-py-pyuv","title":"Python Tool <code>!py</code> <code>!pyuv</code>","text":"<p><code>!py</code> prompts the AI to write a Python script and execute it with your system Python or virtual env (<code>.venv</code>) if available in the current working directory. It may use dependencies that you do not have installed: you can either install them, try repeat <code>!</code> to see if the AI can work around it, or use <code>!pyuv</code> described below.</p> <p><code>!pyuv</code> prompts the AI to write Python with script dependencies so that dependencies can be automatically installed. It requires <code>uv</code>.</p> <p>Here's an example of it in action:</p> <pre><code>[0]: !pyuv distance from sf to nyc\n</code></pre> <pre><code>\u2193\u2193\u2193\n\n# /// script\n# dependencies = [\"geopy\"]\n# ///\nfrom geopy.distance import geodesic\n\n# Coordinates: (lat, lon)\nsf = (37.7749, -122.4194)\nnyc = (40.7128, -74.0060)\n\ndistance_km = geodesic(sf, nyc).kilometers\ndistance_miles = geodesic(sf, nyc).miles\n\nprint(f\"Distance from San Francisco to New York City: {distance_km:.2f} km ({distance_miles:.2f} miles)\")\n\n\u2699 \u2699 \u2699\n\nDistance from San Francisco to New York City: 4139.15 km (2571.95 miles)\n</code></pre>"},{"location":"repl/tools/#custom-program-tool-cmd","title":"Custom program tool <code>!'&lt;cmd&gt;'</code>","text":"<p>You can prompt the AI to generate the <code>stdin</code> to any program. For example, to use <code>psql</code>:</p> <p><pre><code>[0]: !'psql -U postgres -d my_db' what db users are there?\n</code></pre> <pre><code>\u2193\u2193\u2193\n\nSELECT usename FROM pg_user;\n\n\u2699 \u2699 \u2699\n\n usename\n---------\npostgres\n...\n</code></pre></p> <p>A <code>{file}</code> placeholder can be used to prompt the AI to generate an input file rather than <code>stdin</code>. The below example demonstrates the difference using <code>uv</code>.</p> <pre><code># Uses stdin (-)\n[0]: !'uv run --with geopy -' distance from sf to nyc\n\nOR\n\n# Uses temporary file\n[0]: !'uv run --with geopy {file.py}' distance from sf to nyc\n</code></pre> <pre><code>\u2193\u2193\u2193\n\nfrom geopy.distance import geodesic\n\nsf_coords = (37.7749, -122.4194)  # San Francisco coordinates\nnyc_coords = (40.7128, -74.0060)   # New York City coordinates\n\ndistance = geodesic(sf_coords, nyc_coords).miles\nprint(distance)\n\n\u2699 \u2699 \u2699\n\n2571.9457567914133\n</code></pre> <p>Note that a file extension was specified (e.g. <code>{file.&lt;ext&gt;}</code>) to force the temporary file to have a particular extension and enable syntax highlighting. This is helpful for programs that require input to have a specific extension (<code>uv</code> happens to be one of them, requiring <code>.py</code>).</p> <p>Tip</p> <p>Note that <code>uv</code> is used for demonstrative purposes only and <code>!pyuv</code> would be the recommended tool for using <code>uv</code>.</p>"},{"location":"repl/tools/#html-tool-html","title":"HTML tool <code>!html</code>","text":"<p>Even a terminal jockey needs to touch grass every now and then.</p> <p>To prompt the AI to help you visualize output, you can ask for HTML output that will be opened by your configured system-default browser. Asking for iterations using the <code>!html</code> tool will trigger reloads automatically.</p> <p></p>"},{"location":"repl/tools/#clipboard-tool-clip","title":"Clipboard tool <code>!clip</code>","text":"<p>Are you a caveman reaching over to your mouse to copy-and-paste from the terminal? Use <code>!clip</code> to ask the AI to copy-and-paste whatever your need. For example:</p> <pre><code>!clip option #3\n</code></pre>"},{"location":"repl/tools/#hai-tool-hai","title":"hai tool <code>!hai</code>","text":"<p>The LLM knows how to use the REPL.</p> <p><pre><code>[7]: !hai load silk road wiki. who was the founder?\n</code></pre> <pre><code>\u2193\u2193\u2193\n\n- /load-url https://en.wikipedia.org/wiki/Silk_Road_(marketplace)\n- /prompt Who was the founder of the Silk Road marketplace?\n\n\u2699 \u2699 \u2699\n\nPushed 2 command(s) into queue\n</code></pre> <pre><code>!hai-tool[0]: /load-url https://en.wikipedia.org/wiki/Silk_Road_(marketplace)\n</code></pre> <pre><code>Loaded (markdown): Silk Road (marketplace) (32,696 tokens)\n</code></pre> <pre><code>!hai-tool[1]: /prompt Who was the founder of the Silk Road marketplace?\n</code></pre></p> <pre><code>\u2193\u2193\u2193\n\nThe founder of the Silk Road marketplace was **Ross Ulbricht**, who operated under the pseudonym \"Dread Pirate Roberts.\"\n</code></pre> <p>The AI is capable enough to know what URL to load and then ask itself a question about the contents.</p>"},{"location":"repl/tools/#hai-recursion","title":"!hai recursion","text":"<p>Some LLMs (<code>sonnet-3.7</code> is most reliable) are able to work out whether they need to re-prompt themselves to act on intermediate information.</p> <p>In this example, only the first command is prompted by the user. The rest, prefixed by <code>hai-tool[ ]:</code>, are prompted by the AI. For brevity, user confirmations are omitted.</p> <p><pre><code>[0]: !hai get a random number. if it is even, ask the AI how large the sun is. if it is odd, delete `file1.txt`.\n</code></pre> <pre><code>\u2193\u2193\u2193\n\n- !py Generate a random number and determine if it's even or odd\n- !hai Based on the random number generated, if it's even, ask how large the sun is. If it's odd, delete file1.txt\n</code></pre></p> <pre><code>\u2699 \u2699 \u2699\n\nPushed 2 command(s) into queue\n</code></pre> <p><pre><code>hai-tool[3]: !py Generate a random number and determine if it's even or odd\n</code></pre> <pre><code>\u2193\u2193\u2193\n\nimport random\n\n# Generate a random number between 1 and 100\nrandom_number = random.randint(1, 100)\n\n# Check if the number is even or odd\nif random_number % 2 == 0:\n    result = \"even\"\nelse:\n    result = \"odd\"\n\nprint(f\"The random number is: {random_number}\")\nprint(f\"The number is {result}\")\n</code></pre> <pre><code>\u2699 \u2699 \u2699\n\nThe random number is: 86\nThe number is even\n</code></pre></p> <p><pre><code>hai-tool[6]: !hai Based on the random number generated, if it's even, ask how large the sun is. If it's odd, delete file1.txt\n</code></pre> <pre><code>\u2193\u2193\u2193\n\n- /prompt How large is the sun? Please provide information about its diameter, volume, and mass compared to Earth.\n</code></pre> <pre><code>\u2699 \u2699 \u2699\n\nPushed 1 command(s) into queue\n</code></pre></p> <p><pre><code>hai-tool[9]: /prompt How large is the sun? Please provide information about its diameter, volume, and mass compared to Earth.\n</code></pre> <pre><code>\u2193\u2193\u2193\n\n# The Size of the Sun\n\nThe Sun is enormously large compared to Earth. Here are the key measurements:\n\n- Sun's diameter: approximately 1.39 million kilometers (864,000 miles)\n- Sun's volume: approximately 1.41 \u00d7 10^18 cubic kilometers\n- Sun's mass: approximately 1.989 \u00d7 10^30 kilograms\n</code></pre></p>"},{"location":"repl/tools/#function-tool-fn-","title":"Function tool <code>!fn-*</code>","text":"<p>A drawback of <code>!py</code> and <code>!sh</code> is that there's no ability to re-use the code that was written. The LLM writes code every time the tool is invoked, which can consume a lot of output tokens and the time it takes to generate them.</p> <p>For example, computing the square root of pi:</p> <pre><code>[0] !py sqrt pi\n</code></pre> <pre><code>\u2193\u2193\u2193\n\nimport math\nprint(math.sqrt(math.pi))\n</code></pre> <pre><code>\u2699 \u2699 \u2699\n\n1.7724538509055159\n</code></pre> <p>A second request to compute the square root of e requires the code to be rewritten:</p> <pre><code>[3] !py sqrt e\n</code></pre> <pre><code>\u2193\u2193\u2193\n\nimport math\nprint(math.sqrt(math.e))\n</code></pre> <pre><code>\u2699 \u2699 \u2699\n\n1.6487212707001282\n</code></pre> <p>The solution is to use <code>!fn-py &lt;prompt&gt;</code> which instructs the AI to generate a Python function that takes an argument and prints the result to add it to the conversation.</p> <p><pre><code>[0]: !fn-py find the sqrt, print the result\n</code></pre> <pre><code>\u2193\u2193\u2193\n\ndef f(arg):\n    import math\n    result = math.sqrt(arg)\n    print(result)\n</code></pre></p> <pre><code>\u2699 \u2699 \u2699\n\nStored as command: /f0\n</code></pre> <p>The new function can be invoked like a command: <code>/f0 &lt;arg&gt;</code> without prompting the LLM. For example:</p> <pre><code>[3]: /f0 64\n8.0\n</code></pre> <p>The LLM can invoke <code>/f0</code> when you use the <code>!hai</code> tool:</p> <pre><code>[5]: !hai what's the sqrt of pi\n</code></pre> <pre><code>\u2193\u2193\u2193\n\n- /f0 3.141592653589793\n</code></pre> <p><pre><code>\u2699 \u2699 \u2699\n\nPushed 1 command(s) into queue\n</code></pre> <pre><code>!hai-tool[0]: /f0 3.141592653589793\n</code></pre> <pre><code>1.7724538509055159\n</code></pre></p> <p>For a reusable shell function, use <code>!fn-sh</code>. For a resuable Python function that can declare script dependencies, use <code>!fn-pyuv</code>.</p>"},{"location":"repl/tools/#use-native-expressions-as-function-args","title":"Use native expressions as function args","text":"<p>For <code>!fn-py</code> and <code>!fn-pyuv</code>, the arg can be any Python expression.</p> <p><pre><code>[0]: !fn-py distance from san francisco. input is tuple of geo coordinates. print result.\n</code></pre> <pre><code>\u2193\u2193\u2193\n\ndef f(arg):\n    # arg is a tuple of (latitude, longitude)\n    from math import radians, sin, cos, sqrt, atan2\n    # San Francisco coordinates\n    sf_lat, sf_lon = 37.7749, -122.4194\n    lat1, lon1 = sf_lat, sf_lon\n    lat2, lon2 = arg\n    # Radius of the Earth in kilometers\n    R = 6371.0\n    # Convert degrees to radians\n    dlat = radians(lat2 - lat1)\n    dlon = radians(lon2 - lon1)\n    a = sin(dlat / 2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n    distance = R * c\n    print(f\"Distance from San Francisco, CA to {arg}: {distance:.2f} km\")\n</code></pre> <pre><code>\u2699 \u2699 \u2699\n\nStored as command: /f0\n</code></pre></p> <p>Using a Python tuple as the arg:</p> <pre><code>[3]: /f0 (0.0, 78.5)\n</code></pre> <pre><code>Distance from San Francisco, CA to (0.0, 78.5): 15299.16 km\n</code></pre> <p>Similarly, <code>!fn-sh</code> accepts a shell expression.</p>"},{"location":"repl/tools/#named-functions","title":"Named functions","text":"<p>Use the <code>name</code> option to give a function a proper name:</p> <pre><code>[0] !fn-py(name=\"sqrt\") find the sqrt\n</code></pre> <p><pre><code>\u2193\u2193\u2193\n\ndef f(arg):\n    import math\n    return math.sqrt(arg)\n</code></pre> <pre><code>\u2699 \u2699 \u2699\n\nStored as command: /f_sqrt\n</code></pre></p>"},{"location":"repl/tools/#show-all-functions","title":"Show all functions","text":"<p>To see all functions declared in the current conversation, use:</p> <pre><code>/fns\n</code></pre>"}]}