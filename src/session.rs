use std::collections::{HashMap, VecDeque};
use std::env;
use std::sync::Arc;
use tokio::sync::Mutex;
use tokio_util::sync::CancellationToken;
use uuid::Uuid;

use crate::{
    api::client::HaiClient,
    chat, cmd, config,
    db::{self, LogEntryRetentionPolicy},
    tool,
};

#[derive(Clone, Debug)]
pub enum ReplMode {
    Normal,
    /// Enter task-mode for task with given fqn.
    /// Option<String> is the task key for namespacing caching.
    /// bool is whether task is trusted (if so, confirmations skipped)
    Task(String, Option<String>, bool),
}

#[derive(Clone, Debug)]
pub enum HaiRouterState {
    On,
    OffForModel,
    Off,
}

#[derive(Debug)]
pub enum CmdSource {
    /// Input from hai-bye commands
    HaiBye(u32),
    /// Input generated by internal logic rather than user
    Internal,
    /// User input from the REPL
    User,
    /// Input generated by AI usage of !hai tool
    HaiTool(u32),
    // task_signature: (task_name, task_key, task_step_id)
    TaskStep(String, Option<String>, u32),
    // Input generated from listen queue (queue_name, index)
    ListenQueue(Option<String>, u32),
}

impl CmdSource {
    pub fn get_task_step_signature(&self) -> Option<(String, Option<String>, u32)> {
        if let CmdSource::TaskStep(task_name, task_key, task_step_id) = self {
            Some((task_name.clone(), task_key.clone(), *task_step_id))
        } else {
            None
        }
    }
}

#[derive(Debug)]
pub struct CmdInput {
    pub input: String,
    pub source: CmdSource,
}

#[derive(Debug)]
pub struct AiDefinedFn {
    pub fn_def: String,
    pub fn_tool: tool::FnTool,
}

#[derive(Debug)]
pub struct SessionState {
    pub repl_mode: ReplMode,
    /// AI model in active use
    pub ai: config::AiModel,
    pub ai_temperature: Option<f32>,
    /// Running counter of tokens in convo (does not include loaded tokens)
    pub input_tokens: u32,
    /// Running counter of tokens loaded from files (this is retained on /reset)
    pub input_loaded_tokens: u32,
    /// Queue of cmds to run
    pub cmd_queue: VecDeque<CmdInput>,
    /// History stores previous messages
    pub history: Vec<db::LogEntry>,
    /// The program to use to edit assets
    pub editor: String,
    /// The shell to use for the !sh tool.
    pub shell: String,
    /// These are outputs that should be masked due to sensitivity, which means
    /// they were acquired by user input with secret=true. They are sorted by
    /// length descending in case a mask is a left-aligned substring of
    /// another. NOTE: These are not cleared even across conversations.
    pub masked_strings: Vec<String>,
    pub mask_secrets: bool,
    /// Information about logged-in account
    pub account: Option<db::Account>,
    /// Whether the session is in incognito mode (history-less)
    pub incognito: bool,
    /// The last tool that was used (for ! shortcut)
    pub last_tool_cmd: Option<cmd::ToolCmd>,
    /// The tool activated in tool-mode
    pub tool_mode: Option<cmd::ToolModeCmd>,
    /// Whether to use hai-router for compatible AI models
    pub use_hai_router: HaiRouterState,
    /// (Temporary asset file, is task step?)
    pub temp_files: Vec<(tempfile::NamedTempFile, bool)>,
    /// Tools defined by the AI: name -> (Fn def, is task step?).
    pub ai_defined_fns: HashMap<String, (AiDefinedFn, bool)>,
    /// Add new message if conversation has a day transition
    pub add_msg_on_new_day: bool,
    /// (Temp file for HTML output, is task step?, socket address for
    /// websocket server, connected clients, cancellation token to shutdown
    /// server)
    pub html_output: Option<(
        tempfile::NamedTempFile,
        bool,
        std::net::SocketAddr,
        crate::feature::html_tool::Clients,
        CancellationToken,
    )>,
    /// Quick index vars ($0...$N-1)
    pub quick_index_vars: Vec<String>,
}

impl SessionState {
    /// Recalculates token count based on history.
    ///
    /// Useful when history has been pruned.
    pub fn recalculate_input_tokens(&mut self) {
        let mut input_tokens = 0;
        let mut input_loaded_tokens = 0;
        for log_entry in &self.history {
            if log_entry.retention_policy.1 == LogEntryRetentionPolicy::ConversationLoad {
                input_loaded_tokens += log_entry.tokens;
            } else {
                input_tokens += log_entry.tokens;
            }
        }
        self.input_tokens = input_tokens;
        self.input_loaded_tokens = input_loaded_tokens;
    }

    /// Adds a new masked string to the session.
    ///
    /// Use this to maintain the invariant that masked strings are sorted by
    /// length in descending order.
    pub fn add_masked_string(&mut self, s: &str) {
        let masked_string = s.to_string();
        if !self.masked_strings.contains(&masked_string) {
            self.masked_strings.push(masked_string);
            // Sort to maintain invariant that longer strings are first.
            self.masked_strings
                .sort_by_key(|b| std::cmp::Reverse(b.len()));
        }
    }
}

/// Convenience function to add "user text" into conversation history while
/// making the appropriate modifications to the session and token count.
///
/// # Returns
///
/// The number of tokens in `contents`.
pub fn session_history_add_user_text_entry(
    contents: &str,
    session: &mut SessionState,
    bpe_tokenizer: &tiktoken_rs::CoreBPE,
    retention_policy: (bool, LogEntryRetentionPolicy),
) -> u32 {
    let asset_tokens = bpe_tokenizer.encode_with_special_tokens(contents);
    let token_count = asset_tokens.len() as u32;
    if matches!(
        retention_policy.1,
        LogEntryRetentionPolicy::ConversationLoad
    ) {
        session.input_loaded_tokens += token_count;
    } else {
        session.input_tokens += token_count;
    }
    session.history.push(db::LogEntry {
        uuid: Uuid::now_v7().to_string(),
        ts: chrono::Local::now(),
        message: chat::Message {
            role: chat::MessageRole::User,
            content: vec![chat::MessageContent::Text {
                text: contents.to_string(),
            }],
            tool_calls: None,
            tool_call_id: None,
        },
        tokens: token_count,
        retention_policy,
    });
    token_count
}

/// Similar to `session_history_add_user_text_entry` but also adds an entry for
/// the user's input command.
pub fn session_history_add_user_cmd_and_reply_entries(
    cmd: &str,
    contents: &str,
    session: &mut SessionState,
    bpe_tokenizer: &tiktoken_rs::CoreBPE,
    retention_policy: (bool, LogEntryRetentionPolicy),
) -> u32 {
    session_history_add_user_text_entry(cmd, session, bpe_tokenizer, retention_policy)
        + session_history_add_user_text_entry(contents, session, bpe_tokenizer, retention_policy)
}

/// Convenience function to add "user image" into conversation history while
/// making the appropriate modifications to the session and token count.
///
/// FIXME: Token count assumes provider is OpenAI (not necssarily) which will
/// downscale in low-detail mode to 85 tokens. No other provider does this and
/// this count will be inaccurate for them. Either fix token counting or apply
/// resize client-side.
///
/// # Returns
///
/// The number of tokens added.
pub fn session_history_add_user_image_entry(
    img_png_b64: &str,
    session: &mut SessionState,
    retention_policy: (bool, LogEntryRetentionPolicy),
) -> u32 {
    // OpenAI-specific for low-detail images
    let token_count = 85u32;
    if matches!(
        retention_policy.1,
        LogEntryRetentionPolicy::ConversationLoad
    ) {
        session.input_loaded_tokens += token_count;
    } else {
        session.input_tokens += token_count;
    }
    session.history.push(db::LogEntry {
        uuid: Uuid::now_v7().to_string(),
        ts: chrono::Local::now(),
        message: chat::Message {
            role: chat::MessageRole::User,
            content: vec![chat::MessageContent::ImageUrl {
                image_url: chat::ImageData {
                    detail: "low".to_string(),
                    url: format!("data:image/png;base64,{}", &img_png_b64),
                },
            }],
            tool_calls: None,
            tool_call_id: None,
        },
        tokens: token_count,
        retention_policy,
    });
    token_count
}

// --

/// Attempts to activate the hai-router.
///
/// May not be possible due to the AI model. If so, it puts the hai-router into
/// a special state so that it will be activated if the model is switched to
/// one that is supported.
pub fn hai_router_try_activate(session: &mut SessionState) {
    session.use_hai_router = if config::is_ai_model_supported_by_hai_router(&session.ai) {
        HaiRouterState::On
    } else {
        HaiRouterState::OffForModel
    };
}

/// Uses `hai_router_try_activate` for special handling.
pub fn hai_router_set(session: &mut SessionState, on: bool) {
    if on {
        hai_router_try_activate(session);
    } else {
        session.use_hai_router = HaiRouterState::Off;
    }
}

// --

pub async fn account_login_setup_session(
    session: &mut SessionState,
    db: Arc<Mutex<rusqlite::Connection>>,
    user_id: &str,
    username: &str,
    token: &str,
) {
    db::login_account(&*db.lock().await, user_id, username, token)
        .expect("failed to write login info");
    session.account = Some(db::Account {
        user_id: user_id.to_string(),
        username: username.to_string(),
        token: token.to_string(),
    });
    match db::get_misc_entry(&*db.lock().await, username, "hai-router") {
        Ok(Some((hai_router_value, _))) => {
            hai_router_set(session, hai_router_value == "on");
        }
        Ok(_) => {}
        Err(e) => {
            eprintln!("failed to read db: {}", e);
        }
    }
}

pub async fn account_nobody_setup_session(
    session: &mut SessionState,
    db: Arc<Mutex<rusqlite::Connection>>,
) {
    if let Some(cur_account) = &session.account {
        db::switch_to_nobody_account(&*db.lock().await, &cur_account.username)
            .expect("failed to write login info");
        session.account = None;
        hai_router_set(session, false);
    }
}

// --

pub fn mk_api_client(session: Option<&SessionState>) -> HaiClient {
    let mut client = HaiClient::new(&get_api_base_url());
    if let Some(session) = session
        && let Some(ref account) = session.account
    {
        client.set_token(&account.token);
    }
    client
}

pub fn get_api_base_url() -> String {
    match env::var("HAI_BASE_URL") {
        Ok(value) => value,
        _ => "https://hai.superego.ai/1".to_string(),
    }
}

pub fn get_web_base_url() -> String {
    get_api_base_url()[..get_api_base_url().len() - 2].to_string()
}

// --

/// Abridges history in three ways:
/// 1. Only includes User and Assistant messages.
/// 2. Truncates each message to the first 100 characters.
/// 3. Limits the total number of messages to 10.
pub fn get_abridged_history(history: &[db::LogEntry]) -> String {
    let mut result = String::new();
    let mut count = 0;

    for entry in history.iter().filter(|entry| {
        matches!(
            entry.message.role,
            chat::MessageRole::User | chat::MessageRole::Assistant
        )
    }) {
        if count >= 10 {
            break;
        }

        let message = &entry.message;

        let role_str = match message.role {
            chat::MessageRole::User => "User",
            chat::MessageRole::Assistant => "Assistant",
            // These roles are filtered out earlier
            _ => continue,
        };

        // Extract text content from the message
        let content_str = message
            .content
            .iter()
            .map(|content| match content {
                chat::MessageContent::Text { text } => text.clone(),
                chat::MessageContent::ImageUrl { .. } => "[Image]".to_string(),
            })
            .collect::<Vec<String>>()
            .join(" ");

        // Take the first 100 chars (or less if the message is shorter)
        let truncated_content = if content_str.chars().count() > 100 {
            let truncated: String = content_str.chars().take(100).collect();
            format!("{}...", truncated)
        } else {
            content_str
        };

        // Add delimiter if this isn't the first message
        if count > 0 {
            result.push_str("\n\n");
        }

        // Add the formatted message
        result.push_str(&format!("{}: {}", role_str, truncated_content));

        count += 1;
    }

    result
}
